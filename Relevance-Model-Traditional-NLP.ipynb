{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook attempts to fit traditional NLP techniques - SVD, Trigrams etc. - to the economic data newspaper set to draw signal for the relevance dataset. To build robust trigram models, smoothening and discounting are used on the maximum likelihood statistics drawn to balance probability estimates for binary classification. The SVD attempts simple clustering mechanisms to find underlying similarities between article snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pickle.load(open('./data/relevance_trainX.pkl', \"rb\"))\n",
    "trainY = pickle.load(open('./data/relevance_trainY.pkl', \"rb\"))\n",
    "testX = pickle.load(open('./data/relevance_testX.pkl', \"rb\"))\n",
    "testY = pickle.load(open('./data/relevance_testY.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Supervised Learning - Trigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two trigram models $ M_{+} $ and $ M_{-} $ such that every sentence $ S_{i} $, the probability of relevance is given by a softmax activation, namely $\\frac{P_{M+}(S_{i})}{\\sum P(S_{i})} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token cleaning: Stemming, <START> and <STOP>, <NUM> tag\n",
    "def num_tag(num):\n",
    "    digits = np.array(['1','2','3','4','5','6','7','8','9','0'])\n",
    "    return sum(np.vectorize(lambda s: s in num)(digits))>0\n",
    "def cleaner_vocab(s):\n",
    "    if s[0]==\"\\'\" and s[-1]==\"\\'\": s = s[1:-1]\n",
    "    if \".\" in s and len(s)!=1: s = s.replace(\".\",\"\")\n",
    "    return s\n",
    "def trigram_text(s):\n",
    "    s = \"<start> <start> \"+s+\" <end>\"\n",
    "    s = np.vectorize(lambda s: \"<num>\" if num_tag(s) else s)(s.split())\n",
    "    return np.vectorize(lambda s: cleaner_vocab(s))(s)\n",
    "trainX = trainX.apply(lambda txt: trigram_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "postrainX = trainX[trainY==1].copy().reset_index(drop=True)\n",
    "negtrainX = trainX[trainY==0].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Trigram Model\n",
    "class LangModel():\n",
    "    def __init__(self, vocab):\n",
    "        self.unigram, self.bigram, self.trigram = {}, {}, {}\n",
    "        self.unigram.update(dict.fromkeys(vocab, 0))\n",
    "        self.bigram.update(dict.fromkeys(vocab, self.unigram.copy()))\n",
    "        self.trigram.update(dict.fromkeys(vocab, self.bigram.copy()))\n",
    "    \n",
    "    def add_experience(self, word1, word2, word3):\n",
    "        self.unigram[word3]+=1\n",
    "        self.bigram[word2][word3]+=1\n",
    "        self.trigram[word1][word2][word3]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating unigram, bigram and trigram probability dictionaries\n",
    "vocab = trainX.apply(lambda s: s).values\n",
    "vocab = [val for x in vocab for val in x]\n",
    "posModel, negModel = LangModel(vocab), LangModel(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define word counts\n",
    "for sent in postrainX:\n",
    "    for i in range(2, len(sent)):\n",
    "        posModel.add_experience(sent[i-2], sent[i-1], sent[i])\n",
    "for sent in negtrainX:\n",
    "    for i in range(2, len(sent)):\n",
    "        negModel.add_experience(sent[i-2], sent[i-1], sent[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Application of Linear Interpolation on Maximum Likelihood Estimates: <p>\n",
    "$ P(w|u,v) = \\lambda_{1} * Q_{ML}(w|u,v) + \\lambda_{2} * Q_{ML}(w|v) + (1 - \\lambda_{1} - \\lambda_{2}) * Q_{ML}(w) $\n",
    "<br\\><br\\>\n",
    "where the  $ Q_{ML} $ refers to traditional maximum likelihood estimates (unigrams, bigrams, and trigrams), and\n",
    "<br\\>\n",
    "$ \\lambda_{1} = \\dfrac {c(u,v)}{c(u,v)+\\gamma} $, \n",
    "$ \\lambda_{2} = (1 - \\lambda_{1}) * \\dfrac {c(v)}{c(v)+\\gamma} $\n",
    "<br\\>\n",
    "where $ \\gamma $ is optimized by maximizing perplexity or minimizing log loss on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing singular smoothing parameter through perplexity maximization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing train set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating test set probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Semi-Supervised Learning - SVD Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
