{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import scipy as sp\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = open(\"News_Category_Dataset_v2.json\", \"rb\")\n",
    "t = rw.readlines()\n",
    "data = []\n",
    "for i in range(len(t)):\n",
    "    data.append(json.loads(t[i].decode(\"utf-8\").replace(\"\\r\\n\", \"\")))\n",
    "data = pd.io.json.json_normalize(data)\n",
    "data.to_csv(\"news_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27.4, 51.8]      91461\n",
       "(2.755, 27.4]     73763\n",
       "(51.8, 76.2]      15348\n",
       "(76.2, 100.6]       356\n",
       "(149.4, 173.8]       90\n",
       "(125.0, 149.4]       52\n",
       "(100.6, 125.0]       42\n",
       "(173.8, 198.2]       20\n",
       "(222.6, 247.0]        4\n",
       "(198.2, 222.6]        4\n",
       "Name: descrp, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaner(s):\n",
    "    rem = re.compile('[^a-zA-Z0-9\\'#-]')\n",
    "    s = rem.sub(\" \", s).lower()\n",
    "    return s\n",
    "data = data[data.headline.notnull()&data.short_description.notnull()]\n",
    "print(len(data))\n",
    "data['descrp'] = data['headline'] + \" \" + data['short_description']\n",
    "data['descrp'] = data['descrp'].apply(lambda s: cleaner(s))\n",
    "data.drop(['authors', 'link', 'headline', 'short_description', 'date'], axis=1, inplace=True)\n",
    "data['descrp'].apply(lambda s: len(s.split())).value_counts(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing out similar category listings: pair[0] is old categories being closed to pair[1] new category\n",
    "newcats = [(['WELLNESS', 'HEALTHY LIVING'], \"WELLNESS\"), (['STYLE & BEAUTY', 'HOME & LIVING', 'STYLE'], \"LIVING\"), \n",
    "           (['PARENTS', 'PARENTING', 'WEDDINGS', 'DIVORCE', 'FIFTY'], \"FAMILY\"), (['CRIME'], \"CRIME\"),\n",
    "           (['WORLD NEWS', 'THE WORLDPOST', 'WORLDPOST', 'POLITICS', 'RELIGION'], \"POLITICS\"), \n",
    "           (['TECH', 'SCIENCE'], \"TECHNOLOGY\"), (['TRAVEL', 'FOOD & DRINK', 'TASTE'], \"FOOD & TRAVEL\"),\n",
    "           (['ARTS & CULTURE', 'CULTURE & ARTS', 'ARTS'], \"CULTURE\"), (['COLLEGE', 'EDUCATION'], \"EDUCATION\"),\n",
    "           (['ENTERTAINMENT', 'COMEDY', 'SPORTS'], \"ENTERTAINMENT\"), (['ENVIRONMENT', 'GREEN'], \"ENVIRONMENT\"),\n",
    "           (['WOMEN', 'QUEER VOICES', 'BLACK VOICES', 'LATINO VOICES'], \"REPRESENTATIVE VOICES\"), \n",
    "           (['BUSINESS', 'MEDIA', 'IMPACT', 'MONEY'], \"BUSINESS\")]\n",
    "for pair in newcats:\n",
    "    data.loc[data.category.isin(pair[0]), \"category\"] = pair[1]\n",
    "data.drop(data[data.category.isin(['GOOD NEWS', 'WEIRD NEWS'])].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us set our maximum sequence length to 60 since that covers about 96.7% of our data\n",
    "data['descrp'] = data['descrp'].apply(lambda s: s.split())\n",
    "data = data[data.descrp.apply(len) < 60]\n",
    "data['descrp'] = data['descrp'].apply(lambda s: s + [\" \"] * (60-len(s)))\n",
    "#Label-Encode the Categories\n",
    "tp = data.category.value_counts().index.values\n",
    "data['response'] = data.category.apply(lambda s: np.where(tp==s)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=50000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data['descrp'].values)\n",
    "wordindex = tokenizer.word_index\n",
    "X = tokenizer.texts_to_sequences(data['descrp'].values)\n",
    "X = np.array([i + [wordindex[' ']] * (60 - len(i)) for i in X])\n",
    "Y = data.response.values.reshape(-1, 1)\n",
    "Y = keras.utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120379, 60), (51591, 60), (120379, 13), (51591, 13))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(np.array(X), np.array(Y), random_state=1, test_size=0.3)\n",
    "trainX.shape, testX.shape, trainY.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mdr():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(50000, 100, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.1))\n",
    "    model.add(LSTM(75, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(LSTM(75, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(LSTM(75, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(Y.shape[1], activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120379 samples, validate on 51591 samples\n",
      "Epoch 1/10\n",
      "120379/120379 [==============================] - 180s 1ms/step - loss: 2.0822 - acc: 0.2802 - val_loss: 1.8027 - val_acc: 0.3599\n",
      "Epoch 2/10\n",
      "120379/120379 [==============================] - 180s 1ms/step - loss: 1.7465 - acc: 0.3859 - val_loss: 1.6865 - val_acc: 0.4266\n",
      "Epoch 3/10\n",
      "120379/120379 [==============================] - 181s 2ms/step - loss: 1.5693 - acc: 0.4652 - val_loss: 1.5426 - val_acc: 0.4840\n",
      "Epoch 4/10\n",
      "120379/120379 [==============================] - 177s 1ms/step - loss: 1.3661 - acc: 0.5399 - val_loss: 1.3612 - val_acc: 0.5714\n",
      "Epoch 5/10\n",
      "120379/120379 [==============================] - 176s 1ms/step - loss: 1.1348 - acc: 0.6473 - val_loss: 1.1894 - val_acc: 0.6485\n",
      "Epoch 6/10\n",
      "120379/120379 [==============================] - 182s 2ms/step - loss: 0.9454 - acc: 0.7190 - val_loss: 1.1076 - val_acc: 0.6780\n",
      "Epoch 7/10\n",
      "120379/120379 [==============================] - 179s 1ms/step - loss: 0.8210 - acc: 0.7575 - val_loss: 1.0807 - val_acc: 0.6889\n",
      "Epoch 8/10\n",
      "120379/120379 [==============================] - 178s 1ms/step - loss: 0.7229 - acc: 0.7878 - val_loss: 1.0866 - val_acc: 0.6926\n",
      "Epoch 9/10\n",
      "120379/120379 [==============================] - 3518s 29ms/step - loss: 0.6413 - acc: 0.8125 - val_loss: 1.0679 - val_acc: 0.6988\n",
      "Epoch 10/10\n",
      "120379/120379 [==============================] - 174s 1ms/step - loss: 0.5680 - acc: 0.8352 - val_loss: 1.0897 - val_acc: 0.6989\n"
     ]
    }
   ],
   "source": [
    "chk = ModelCheckpoint('./djiarnn2.h5', monitor='loss', save_best_only=True, period=10)\n",
    "callbacklist = [chk]\n",
    "mdl = KerasClassifier(build_fn=build_mdr, epochs=10, batch_size=500, verbose=True, callbacks=callbacklist, \n",
    "                      validation_data=(testX, testY))\n",
    "mdl.fit(trainX, trainY)\n",
    "mdl.model.save(\"news.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51591/51591 [==============================] - 18s 352us/step\n"
     ]
    }
   ],
   "source": [
    "res = mdl.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36055"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res==np.array([np.argmax(i) for i in testY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
