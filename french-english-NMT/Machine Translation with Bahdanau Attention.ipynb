{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement Bahdanau et al. (2015)'s attention architecture with an encoder-decoder to translate French and English phrases from the Tatoeba Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import unicodedata\n",
    "import gc\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Activation, Bidirectional, Embedding, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> STEP 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng, frn = collections.deque(), collections.deque()\n",
    "with open(\"./data/fra.txt\",encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        text = line.strip().split(\"\\t\")\n",
    "        eng.append(text[0])\n",
    "        frn.append(text[1])\n",
    "eng, ind = np.unique(eng, return_index=True)\n",
    "frn = np.array(frn)[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    regexp = re.compile(r'\\s+', re.UNICODE)\n",
    "    ns = regexp.sub(' ', string)\n",
    "    ns = re.sub(\"[^a-zA-Z0-9«».,?!\\\"\\']\",\" \",ns)\n",
    "    ns = re.sub(r'([«».,?!\\\"\\'])', r' \\1 ', ns)\n",
    "    return ns.lower()\n",
    "eng = np.vectorize(preprocess)(eng)\n",
    "eng = [elem.split() for elem in eng]\n",
    "frn = np.vectorize(preprocess)(frn)\n",
    "frn = [elem.split() for elem in frn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enforce a 14-word restriction on the set\n",
    "mask = np.array([len(elem)<=14 for elem in eng])\n",
    "mask = mask & np.array([len(elem)<=14 for elem in frn])\n",
    "eng = [eng[i] for i in range(len(eng)) if mask[i]]\n",
    "frn = [frn[i] for i in range(len(frn)) if mask[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "def tokenize(sents):\n",
    "    data = np.zeros((len(sents),16), dtype=np.int64)\n",
    "    word_to_index, index_to_word = {\"<begin>\":1,\"<end>\":2}, {1:\"<begin>\", 2:\"<end>\"}\n",
    "    curindex = 3\n",
    "    for i in range(len(sents)):\n",
    "        data[i,0] = 1\n",
    "        for j in range(len(sents[i])):\n",
    "            if word_to_index.get(sents[i][j], None) is None:\n",
    "                word_to_index[sents[i][j]] = curindex\n",
    "                index_to_word[curindex] = sents[i][j]\n",
    "                curindex+=1\n",
    "            data[i,j+1] = word_to_index[sents[i][j]]\n",
    "        data[i,len(sents[i])+1] = word_to_index[\"<end>\"]\n",
    "    return data, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90000, 16), (26084, 16), (90000, 16), (26084, 16))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Complete Tokenization and Create train-test sets\n",
    "engdata, engword_to_index, engindex_to_word = tokenize(eng)\n",
    "frndata, frnword_to_index, frnindex_to_word = tokenize(frn)\n",
    "train_eng, test_eng = engdata[:90000], engdata[90000:]\n",
    "train_frn, test_frn = frndata[:90000], frndata[90000:]\n",
    "train_eng.shape, test_eng.shape, train_frn.shape, test_frn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> STEP 2: Designing Encoder, Decoder, and Attention Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 16, 256)           3398144   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 16, 512), (None,  1574912   \n",
      "=================================================================\n",
      "Total params: 4,973,056\n",
      "Trainable params: 4,973,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_encoder():\n",
    "    tf.keras.backend.clear_session()\n",
    "    inp = Input((16,))\n",
    "    embed = Embedding(len(engword_to_index)+1, 256, embeddings_initializer=\"uniform\")\n",
    "    rep = embed(inp)\n",
    "    encoding, hidden_h, hidden_c = LSTM(512, return_sequences=True, return_state=True)(rep)\n",
    "    return Model(inputs=inp, outputs=[encoding, hidden_h, hidden_c], name=\"Encoder\")\n",
    "build_encoder().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Bahdanau-Attention\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 16, 512)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 512)]     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16, 512)      262656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 512)       262656      tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add (TensorFlowOpLa [(None, 16, 512)]    0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 512)      0           tf_op_layer_add[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16, 1)        513         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(None, 1, 16)]      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax (TensorFlow [(None, 1, 16)]      0           tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_1 (Tensor [(None, 16, 1)]      0           tf_op_layer_Softmax[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul (TensorFlowOpLa [(None, 16, 512)]    0           tf_op_layer_transpose_1[0][0]    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 512)]        0           tf_op_layer_mul[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 525,825\n",
      "Trainable params: 525,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_attention():\n",
    "    tf.keras.backend.clear_session()\n",
    "    #Take in inputs from encoder\n",
    "    enc_output = Input((16,512))\n",
    "    hidden_h = Input((512,))\n",
    "    #expand dims to broadcast to the output shape\n",
    "    hidden = tf.expand_dims(hidden_h, axis=1)\n",
    "    #Define the attention layer's sub-layers\n",
    "    dense1 = Dense(units=512, activation=None)\n",
    "    dense2 = Dense(units=512, activation=None)\n",
    "    mid = Activation(activation=\"tanh\")\n",
    "    final = Dense(units=1, activation=None)\n",
    "    #Calculate score and attention matrix\n",
    "    score = final(mid(dense1(enc_output)+dense2(hidden)))\n",
    "    attmatrix = tf.nn.softmax(score, axis=1)\n",
    "    vector = tf.reduce_sum(attmatrix * enc_output, axis=1)\n",
    "    return Model(inputs=[enc_output, hidden_h], outputs=[vector, attmatrix], name=\"Bahdanau-Attention\")\n",
    "build_attention().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 16, 512)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Bahdanau-Attention (Model)      [(None, 512), (None, 525825      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 512)]     0           Bahdanau-Attention[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 256)       4503808     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 1, 768)]     0           tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 1, 512), (No 2623488     tf_op_layer_concat[0][0]         \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 512)]        0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 17593)        9025209     tf_op_layer_Squeeze[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 16,678,330\n",
      "Trainable params: 16,678,330\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_decoder(attlayer):\n",
    "    tf.keras.backend.clear_session()\n",
    "    #Read in Encoder and previous-prediction Decoder input\n",
    "    enc_output = Input((16,512))\n",
    "    hidden_h, hidden_c = Input((512,)), Input((512,))\n",
    "    prevpred = Input((1,))\n",
    "    #Run Bahdanau Attention\n",
    "    vector, attmatrix = attlayer([enc_output, hidden_h])\n",
    "    #Extract the French Embedding\n",
    "    embed = Embedding(len(frnword_to_index)+1, 256, embeddings_initializer=\"uniform\")\n",
    "    rep = embed(prevpred)\n",
    "    rep = tf.concat([tf.expand_dims(vector, axis=1), rep], axis=2)\n",
    "    #Run a Forward LSTM\n",
    "    recur = LSTM(512, return_sequences=True, return_state=True)\n",
    "    pred, newhidden_h, newhidden_c = recur(rep, initial_state=[hidden_h, hidden_c])\n",
    "    pred = tf.squeeze(pred, [1])\n",
    "    #Predict Next Word\n",
    "    pred = Dense(len(frnword_to_index)+1)(pred)\n",
    "    return Model(inputs=[enc_output, hidden_h, hidden_c, prevpred], \n",
    "                 outputs=[pred, newhidden_h, newhidden_c, attmatrix], name=\"Decoder\")\n",
    "build_decoder(build_attention()).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> STEP 3: Build Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "def loss(true, pred):\n",
    "    ls = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "    ls = ls(true, pred)\n",
    "    print(ls)\n",
    "    temptrue = tf.cast(true, tf.float32)\n",
    "    ls = tf.where(tf.math.equal(temptrue, 0.0), 0.0, ls)\n",
    "    return tf.reduce_mean(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build all model graphs\n",
    "encoder = build_encoder()\n",
    "attnlayer = build_attention()\n",
    "decoder = build_decoder(attnlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def batch_trainer(engsent, frnsent):\n",
    "    batchloss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoutput, hh, hc = encoder(engsent)\n",
    "        prevpred = tf.expand_dims(frnsent[:,0], axis=1)\n",
    "        for i in range(1,frnsent.shape[1]):\n",
    "            pred, hh, hc, _ = decoder([encoutput, hh, hc, prevpred])\n",
    "            batchloss+=loss(frnsent[:,i], pred)\n",
    "            prevpred = tf.expand_dims(frnsent[:,i], axis=1)\n",
    "        batchloss /= frnsent.shape[1]\n",
    "        grads = tape.gradient(batchloss, encoder.trainable_variables+decoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder.trainable_variables+decoder.trainable_variables))\n",
    "    return batchloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numepochs=100\n",
    "batchsize = 128\n",
    "trainedges = np.arange(0, train_eng.shape[0]+batchsize, batchsize)\n",
    "trainloss = collections.deque()\n",
    "for epoch in range(numepochs):\n",
    "    eptrain = 0\n",
    "    for i in range(len(trainedges)-1):\n",
    "        eptrain+=batch_trainer(train_eng[trainedges[i]:trainedges[i+1]], \n",
    "                               train_frn[trainedges[i]:trainedges[i+1]])\n",
    "    trainloss.append(eptrain/(len(trainedges)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trainloss, open(\"./data/trainloss.pkl\",\"wb\"))\n",
    "encoder.save(\"./data/encoder.h5\")\n",
    "decoder.save(\"./data/decoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>STEP 4: Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_weights(\"./data/encoder.h5\")\n",
    "decoder.load_weights(\"./data/decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(engsent, frnsent):\n",
    "    #Set up the sentence prediction matrix\n",
    "    predfrnsent = np.zeros(frnsent.shape, dtype=np.int64)\n",
    "    predfrnsent[:,0] = frnsent[:,0]\n",
    "    #Set up the attention matrix\n",
    "    frn_attn_matrix = np.zeros((frnsent.shape[0], engsent.shape[1], frnsent.shape[1]))\n",
    "    encoutput, hc, hh = encoder.predict(engsent)\n",
    "    prevpred = deepcopy(frnsent[:,0]).reshape(-1,1)\n",
    "    for i in range(1,frnsent.shape[1]):\n",
    "        pred, hh, hc, attmatrix = decoder.predict([encoutput, hh, hc, prevpred])\n",
    "        predfrnsent[:,i] = np.argmax(pred, axis=1)\n",
    "        prevpred = predfrnsent[:,i].reshape(-1,1)\n",
    "        frn_attn_matrix[:,:,i] = attmatrix.reshape(-1,16)\n",
    "    return predfrnsent, frn_attn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(sent, index_to_word):\n",
    "    ret = collections.deque()\n",
    "    for i in range(sent.shape[0]):\n",
    "        phrase = \"\"\n",
    "        for j in range(sent.shape[1]):\n",
    "            phrase+=index_to_word[sent[i,j]]+\" \"\n",
    "            if index_to_word[sent[i,j]]==\"<end>\":\n",
    "                break\n",
    "        ret.append(phrase)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(test_frn.shape, dtype=np.int64)\n",
    "batchsize = 256\n",
    "edges = np.arange(0,  pred.shape[0]+batchsize, batchsize)\n",
    "for i in range(len(edges)-1):\n",
    "    pred[edges[i]:edges[i+1]] = evaluator(test_eng[edges[i]:edges[i+1]],\n",
    "                                         test_frn[edges[i]:edges[i+1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedsent = get_sentences(pred, frnindex_to_word)\n",
    "truesent = get_sentences(test_frn, frnindex_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
