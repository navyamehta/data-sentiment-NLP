{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement Bahdanau et al. (2015)'s attention architecture with an encoder-decoder to translate French and English phrases from the Tatoeba Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import unicodedata\n",
    "import gc\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Activation, Bidirectional, Embedding, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> STEP 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng, frn = collections.deque(), collections.deque()\n",
    "with open(\"./data/fra.txt\",encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        text = line.strip().split(\"\\t\")\n",
    "        eng.append(text[0])\n",
    "        frn.append(text[1])\n",
    "eng, ind = np.unique(eng, return_index=True)\n",
    "frn = np.array(frn)[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    regexp = re.compile(r'\\s+', re.UNICODE)\n",
    "    ns = regexp.sub(' ', string)\n",
    "    ns = re.sub(\"[^a-zA-Z0-9«».,?!\\\"\\']\",\" \",ns)\n",
    "    ns = re.sub(r'([«».,?!\\\"\\'])', r' \\1 ', ns)\n",
    "    return ns.lower()\n",
    "eng = np.vectorize(preprocess)(eng)\n",
    "eng = [elem.split() for elem in eng]\n",
    "frn = np.vectorize(preprocess)(frn)\n",
    "frn = [elem.split() for elem in frn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enforce a 14-word restriction on the set\n",
    "mask = np.array([len(elem)<=14 for elem in eng])\n",
    "mask = mask & np.array([len(elem)<=14 for elem in frn])\n",
    "eng = [eng[i] for i in range(len(eng)) if mask[i]]\n",
    "frn = [frn[i] for i in range(len(frn)) if mask[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "def tokenize(sents):\n",
    "    data = np.zeros((len(sents),16), dtype=np.int64)\n",
    "    word_to_index, index_to_word = {\"<begin>\":1,\"<end>\":0}, {1:\"<begin>\", 0:\"<end>\"}\n",
    "    curindex = 2\n",
    "    for i in range(len(sents)):\n",
    "        data[i,0] = 1\n",
    "        for j in range(len(sents[i])):\n",
    "            if word_to_index.get(sents[i][j], None) is None:\n",
    "                word_to_index[sents[i][j]] = curindex\n",
    "                index_to_word[curindex] = sents[i][j]\n",
    "                curindex+=1\n",
    "            data[i,j+1] = word_to_index[sents[i][j]]\n",
    "    return data, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90000, 16), (26084, 16), (90000, 16), (26084, 16))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Complete Tokenization and Create train-test sets\n",
    "engdata, engword_to_index, engindex_to_word = tokenize(eng)\n",
    "frndata, frnword_to_index, frnindex_to_word = tokenize(frn)\n",
    "train_eng, test_eng = engdata[:90000], engdata[90000:]\n",
    "train_frn, test_frn = frndata[:90000], frndata[90000:]\n",
    "train_eng.shape, test_eng.shape, train_frn.shape, test_frn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> STEP 2: Designing Encoder, Decoder, and Attention Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 16, 256)           3397888   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional [(None, 16, 512), (None,  1050624   \n",
      "=================================================================\n",
      "Total params: 4,448,512\n",
      "Trainable params: 4,448,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_encoder():\n",
    "    tf.keras.backend.clear_session()\n",
    "    inp = Input((16,))\n",
    "    embed = Embedding(len(engword_to_index), 256, embeddings_initializer=\"uniform\")\n",
    "    rep = embed(inp)\n",
    "    recur = Bidirectional(LSTM(256, return_sequences=True, return_state=True), merge_mode=\"concat\")\n",
    "    encoding, fwd_hidden_1, fwd_hidden_2, _, _ = recur(rep)\n",
    "    return Model(inputs=inp, outputs=[encoding, fwd_hidden_1, fwd_hidden_2], name=\"Encoder\")\n",
    "build_encoder().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Bahdanau-Attention\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 512)]        0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 16, 512)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 512)]     0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16, 512)      262656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 512)       262656      tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add (TensorFlowOpLa [(None, 16, 512)]    0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 512)      0           tf_op_layer_add[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16, 1)        513         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(None, 1, 16)]      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax (TensorFlow [(None, 1, 16)]      0           tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_1 (Tensor [(None, 16, 1)]      0           tf_op_layer_Softmax[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul (TensorFlowOpLa [(None, 16, 512)]    0           tf_op_layer_transpose_1[0][0]    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 512)]        0           tf_op_layer_mul[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 525,825\n",
      "Trainable params: 525,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_attention():\n",
    "    tf.keras.backend.clear_session()\n",
    "    #Take in inputs from encoder\n",
    "    enc_output = Input((16,512))\n",
    "    hidden_1, hidden_2 = Input((256,)), Input((256,))\n",
    "    #Concatenate hidden states and expand dims to broadcast to the output shape\n",
    "    hidden = tf.expand_dims(tf.concat([hidden_1, hidden_2], axis=1), axis=1)\n",
    "    #Define the attention layer's sub-layers\n",
    "    dense1 = Dense(units=512, activation=None)\n",
    "    dense2 = Dense(units=512, activation=None)\n",
    "    mid = Activation(activation=\"tanh\")\n",
    "    final = Dense(units=1, activation=None)\n",
    "    #Calculate score and attention matrix\n",
    "    score = final(mid(dense1(enc_output)+dense2(hidden)))\n",
    "    attmatrix = tf.nn.softmax(score, axis=1)\n",
    "    vector = tf.reduce_sum(attmatrix * enc_output, axis=1)\n",
    "    return Model(inputs=[enc_output, hidden_1, hidden_2], outputs=[vector, attmatrix], name=\"Bahdanau-Attention\")\n",
    "build_attention().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 16, 512)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Bahdanau-Attention (Model)      [(None, 512), (None, 525825      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 512)]     0           Bahdanau-Attention[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 256)       4503552     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 1, 768)]     0           tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 1049600     tf_op_layer_concat[0][0]         \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 17592)        4521144     lstm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 10,600,121\n",
      "Trainable params: 10,600,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_decoder(attlayer):\n",
    "    tf.keras.backend.clear_session()\n",
    "    #Read in Encoder and previous-prediction Decoder input\n",
    "    enc_output = Input((16,512))\n",
    "    hidden_1, hidden_2 = Input((256,)), Input((256,))\n",
    "    prevpred = Input((1,))\n",
    "    #Run Bahdanau Attention\n",
    "    vector, attmatrix = attlayer([enc_output, hidden_1, hidden_2])\n",
    "    #Extract the French Embedding\n",
    "    embed = Embedding(len(frnword_to_index), 256, embeddings_initializer=\"uniform\")\n",
    "    rep = embed(prevpred)\n",
    "    rep = tf.concat([tf.expand_dims(vector, axis=1), rep], axis=2)\n",
    "    #Run a Forward LSTM\n",
    "    recur = LSTM(256, return_state=True)\n",
    "    pred, newhidden_1, newhidden_2 = recur(rep, initial_state=[hidden_1, hidden_2])\n",
    "    #Predict Next Word\n",
    "    pred = Dense(len(frnword_to_index), activation=\"sigmoid\")(pred)\n",
    "    return Model(inputs=[enc_output, hidden_1, hidden_2, prevpred], \n",
    "                 outputs=[pred, newhidden_1, newhidden_2, attmatrix], name=\"Decoder\")\n",
    "build_decoder(build_attention()).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> STEP 3: Build Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "def loss(true, pred):\n",
    "    ls = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "    ls = ls(true, pred)\n",
    "    temptrue = tf.cast(true, tf.float32)\n",
    "    ls = tf.where(tf.math.equal(temptrue, 0.0), 0.0, ls)\n",
    "    return tf.reduce_mean(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build all model graphs\n",
    "encoder = build_encoder()\n",
    "attnlayer = build_attention()\n",
    "decoder = build_decoder(attnlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def batch_trainer(engsent, frnsent):\n",
    "    batchloss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoutput, h1, h2 = encoder(engsent)\n",
    "        prevpred = tf.expand_dims(frnsent[:,0], axis=1)\n",
    "        for i in range(1,frnsent.shape[1]):\n",
    "            prevpred, h1, h2, _ = decoder([encoutput, h1, h2, prevpred])\n",
    "            batchloss+=loss(frnsent[:,i], prevpred)\n",
    "            prevpred = tf.expand_dims(tf.argmax(prevpred, axis=-1), axis=1)\n",
    "        grads = tape.gradient(batchloss, encoder.trainable_variables+decoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder.trainable_variables+decoder.trainable_variables))\n",
    "    return batchloss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numepochs=2\n",
    "batchsize = 256\n",
    "edges = np.arange(0, train_eng.shape[0]+batchsize, batchsize)\n",
    "lossq = collections.deque()\n",
    "for epoch in range(numepochs):\n",
    "    eploss = 0\n",
    "    for i in range(len(edges)-1):\n",
    "        eploss+=batch_trainer(train_eng[edges[i]:edges[i+1]], train_frn[edges[i]:edges[i+1]])\n",
    "    lossq.append(eploss/(len(edges)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lossq, open(\"./lossvec.pkl\",\"wb\"))\n",
    "encoder.save(\"./encoder.h5\")\n",
    "decoder.save(\"./decoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>STEP 4: Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([<tf.Tensor: shape=(), dtype=float32, numpy=77.526215>,\n",
       "       <tf.Tensor: shape=(), dtype=float32, numpy=77.38449>])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.526215"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossq[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
