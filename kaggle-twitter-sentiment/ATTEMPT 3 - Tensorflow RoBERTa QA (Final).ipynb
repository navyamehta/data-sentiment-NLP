{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import re\n",
    "import pickle\n",
    "import gc\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "data = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\n",
    "data.dropna(how=\"any\", inplace=True)\n",
    "#Attempt reproducibility\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_cleaner(word):\n",
    "    regex = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    replace = r'\\1\\2\\3'\n",
    "    if word==\"\": return word\n",
    "    if word==\"<3\": return \"LOVE\" #Independent emoticon disambiguation\n",
    "    for i in range(len(word)):\n",
    "        if word[i] in [\"!\",\".\",\"?\",\":\",\",\",\"`\",\"-\",\"=\",\";\"]: return slang_cleaner(\n",
    "            word[:i])+word[i]+slang_cleaner(word[i+1:])\n",
    "    if nltk.corpus.wordnet.synsets(word): return word\n",
    "    clean = regex.sub(replace, word)\n",
    "    if (word==clean): return word\n",
    "    else: return slang_cleaner(clean)\n",
    "def cleaner(sent):\n",
    "    #Two tokens (WEBSITE, VULGAR) are created and punctuation is spaced out\n",
    "    sent = \" \".join(np.vectorize(lambda s:\"WEBSITE\" if \"http\" in s or (\"www\" in s and \"com\" in s) else s)\n",
    "                    (np.array(sent.split())))\n",
    "    for punc in [\"\\!\",\"\\.\",\"\\?\",\"\\:\",\"\\,\",\"\\`\",\"\\-\",\"\\=\",\"\\;\"]:\n",
    "        sent = re.sub(re.compile('(?:'+punc+'){2,}'),punc[1],sent)\n",
    "    sent = re.sub(\"[`]\",\"\\'\",sent)\n",
    "    sent = re.sub(re.compile('(?:\\*){2,}'),\"VULGAR\",sent)\n",
    "    return (\" \".join(np.vectorize(slang_cleaner)(np.array(sent.split())))).lower()\n",
    "for col in ['text','selected_text']:\n",
    "    data[col] = data[col].apply(lambda s: cleaner(s))\n",
    "data = data.loc[data.text.apply(lambda s: len(s))!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validity Checks\n",
    "def validitycheck(text, subtext):\n",
    "    breaks = [\"!\",\".\",\"?\",\":\",\",\",\"`\",\"-\",\"=\",\";\",\" \"]\n",
    "    startpos = text.find(subtext)\n",
    "    endpos = startpos+len(subtext)\n",
    "    if startpos<0: return False\n",
    "    if (startpos!=0) and (text[startpos]!=\" \") and (text[startpos-1] not in breaks): return False\n",
    "    if (endpos!=len(text))  and (text[endpos] not in breaks) and (text[endpos-1] not in breaks): return False\n",
    "    return True\n",
    "data = data.loc[data.apply(lambda s: validitycheck(s.text, s.selected_text), axis=1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Modelling <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labelling(info, training):\n",
    "    input_ids = np.ones((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    attention_mask = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    token_type_ids = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    start_tokens = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    end_tokens = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    for k in range(info.shape[0]):\n",
    "        text1 = \" \"+\" \".join(info.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1) \n",
    "        offsets = []; idx=0\n",
    "        for t in enc.ids:\n",
    "            w = tokenizer.decode([t])\n",
    "            offsets.append((idx,idx+len(w)))\n",
    "            idx += len(w)\n",
    "        s_tok = sentiment_id[info.loc[k,\"sentiment\"]]\n",
    "        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "        attention_mask[k,:len(enc.ids)+5] = 1\n",
    "        if training:\n",
    "            text2 = \" \".join(info.loc[k,'selected_text'].split())\n",
    "            idx = text1.find(text2)\n",
    "            chars = np.zeros((len(text1)))\n",
    "            chars[idx:idx+len(text2)]=1\n",
    "            if text1[idx-1]==' ': chars[idx-1] = 1\n",
    "            toks = []\n",
    "            for i,(a,b) in enumerate(offsets):\n",
    "                sm = np.sum(chars[a:b])\n",
    "                if sm>0: toks.append(i)\n",
    "            if len(toks)>0:\n",
    "                start_tokens[k,toks[0]+1] = 1\n",
    "                end_tokens[k,toks[-1]+2] = 1\n",
    "    return input_ids, attention_mask, token_type_ids, start_tokens, end_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothener(tokenspre):\n",
    "    import scipy.stats as stats\n",
    "    def activated_gauss(s):\n",
    "        loc = s.values[-1]\n",
    "        mod = s.apply(lambda g: stats.norm.pdf(g, loc=loc, scale=0.5))\n",
    "        return mod[:-1]/np.sum(mod[:-1])\n",
    "    tokens = pd.DataFrame(np.tile(np.arange(tokenspre.shape[1]), (tokenspre.shape[0],1)))\n",
    "    tokens['info' ] = np.argmax(tokenspre, axis=1)\n",
    "    tokens = tokens.apply(lambda s: activated_gauss(s), axis=1)\n",
    "    return tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance-Based Entropy\n",
    "def distance_weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    crossentropy = tf.reduce_sum(tf.math.multiply(tf.math.log(y_pred), tf.cast(y_true, dtype=tf.float32)), axis=-1)\n",
    "    distance = tf.abs(tf.math.argmax(y_true, axis=-1)-tf.argmax(y_pred, axis=-1))+1\n",
    "    return -tf.reduce_sum(tf.math.multiply(tf.cast(distance, dtype=tf.float32), crossentropy))\n",
    "#CDF-Loss\n",
    "def CDF_loss(y_true, y_pred):\n",
    "    truelabel = tf.math.cumsum(tf.cast(y_true, dtype=y_pred.dtype), axis=1)\n",
    "    predlabel = tf.math.cumsum(y_pred, axis=1)\n",
    "    return tf.reduce_sum((truelabel-predlabel)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(drp=0.1, l2reg=0.00, activation=None, kinit=\"glorot_uniform\"):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    config = RobertaConfig.from_pretrained(PATH+\"config-roberta-base.json\")\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    x1 = tf.keras.layers.GaussianDropout(drp)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.GaussianDropout(drp)(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    x2 = tf.keras.layers.GaussianDropout(drp)(x[0])\n",
    "    x2 = tf.keras.layers.Conv1D(128, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.GaussianDropout(drp)(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.copy().reset_index(drop=True)\n",
    "input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = generate_labelling(train, True)\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "for i,(train_index,test_index) in enumerate(cv.split(input_ids,train.sentiment.values)):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_model()\n",
    "    save = tf.keras.callbacks.ModelCheckpoint('roberta-fold%i.h5'%(i), monitor='val_loss', verbose=1, \n",
    "                                              save_best_only=True, save_weights_only=True, mode='auto', \n",
    "                                              save_freq='epoch')\n",
    "    model.fit([input_ids[train_index,], attention_mask[train_index,], token_type_ids[train_index,]], \n",
    "              [start_tokens[train_index,], end_tokens[train_index,]], epochs=5, batch_size=64, verbose=True, \n",
    "              validation_data=([input_ids[test_index,],attention_mask[test_index,],token_type_ids[test_index,]], \n",
    "                               [start_tokens[test_index,], end_tokens[test_index,]]),callbacks=[save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predictions on Test Data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neutral Labels\n",
    "testdata = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
    "testdata['original_text'] = testdata['text'].copy()\n",
    "testdata['text'] = testdata['text'].apply(lambda s: cleaner(s))\n",
    "testdata['selected_text'] = np.nan\n",
    "testdata.loc[testdata.sentiment==\"neutral\", \"selected_text\"] = testdata.loc[\n",
    "    testdata.sentiment==\"neutral\", \"original_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SubText Generation Function\n",
    "def subtext(info, input_ids, attention_mask, token_type_ids, tag):\n",
    "    begin, end = np.zeros((len(info), MAX_LEN)), np.zeros((len(info), MAX_LEN))\n",
    "    for fold in range(5):\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = build_model()\n",
    "        model.load_weights(tag+\"roberta-fold\"+str(fold)+\".h5\")\n",
    "        tempbegin, tempend = model.predict([input_ids, attention_mask, token_type_ids])\n",
    "        begin += tempbegin\n",
    "        end += tempend\n",
    "    for k in range(len(info)):\n",
    "        text1 = \" \"+\" \".join(info.loc[info.index[k],'text'].split())\n",
    "        enc, st = tokenizer.encode(text1), None\n",
    "        a = np.argmax(begin[k,])\n",
    "        b = np.argmax(end[k,])\n",
    "        if a>=b:\n",
    "            if np.max(end[k,])>np.max(begin[k,]):\n",
    "                st = tokenizer.decode(enc.ids[:b-1])\n",
    "            else:\n",
    "                st = tokenizer.decode(enc.ids[a-1:])\n",
    "        else:\n",
    "            st = tokenizer.decode(enc.ids[a-1:b-1]) \n",
    "        info.loc[info.index[k], \"selected_text\"] = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Labels\n",
    "postestdata = testdata.loc[testdata.sentiment==\"positive\"].reset_index(drop=True)\n",
    "input_ids, attention_mask, token_type_ids, _, _ = generate_labelling(postestdata, False)\n",
    "subtext(postestdata, input_ids, attention_mask, token_type_ids, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Labels\n",
    "negtestdata = testdata.loc[testdata.sentiment==\"negative\"].reset_index(drop=True)\n",
    "input_ids, attention_mask, token_type_ids, _, _ = generate_labelling(negtestdata, False)\n",
    "subtext(negtestdata, input_ids, attention_mask, token_type_ids, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cohesion (Start and End Checks)\n",
    "def startcorrect(fulltext, subtext):\n",
    "    subtext = \" \".join(subtext.split())\n",
    "    fulltext = \" \"+\" \".join(fulltext.split())+\" \"\n",
    "    i = fulltext.find(subtext)\n",
    "    if i<0: return np.nan\n",
    "    k = i\n",
    "    breaks = [\"!\",\".\",\"?\",\":\",\",\",\"`\",\"-\",\"=\",\";\",\" \"]\n",
    "    if fulltext[i] in breaks: return subtext\n",
    "    while (k!=0) and (fulltext[k-1] not in breaks): k-=1\n",
    "    return fulltext[k:i]+subtext\n",
    "def endcorrect(fulltext, subtext):\n",
    "    subtext = \" \".join(subtext.split())\n",
    "    fulltext = \" \"+\" \".join(fulltext.split())+\" \"\n",
    "    i = fulltext.find(subtext)\n",
    "    if i<0: return np.nan\n",
    "    k=i+len(subtext)\n",
    "    breaks = [\"!\",\".\",\"?\",\":\",\",\",\"`\",\"-\",\"=\",\";\",\" \"]\n",
    "    if fulltext[k-1] in breaks: return subtext\n",
    "    while (k!=len(fulltext)) and (fulltext[k] not in breaks): k+=1\n",
    "    return subtext+fulltext[i+len(subtext):k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cohesion (Clean to Preprocessed Conversion)\n",
    "def reverseprocessing(maintext, pretext, subtext):\n",
    "    maintext = \" \".join(maintext.split())\n",
    "    pretext = \" \".join(pretext.split())\n",
    "    subtext = \" \".join(subtext.split())\n",
    "    if subtext==\"\": return subtext\n",
    "    startpos = maintext.find(subtext)\n",
    "    endpos = startpos+len(subtext)\n",
    "    breakstart_inclpunc, breakend_inclpunc, breakstart_nopunc, breakend_nopunc = False, False, False, False\n",
    "    puncs = [\"!\",\".\",\"?\",\":\",\",\",\"`\",\"-\",\"=\",\";\"]\n",
    "    if (startpos!=0):\n",
    "        if (maintext[startpos-1]!=\" \"):\n",
    "            if maintext[startpos] in puncs:\n",
    "                breakstart_inclpunc = True\n",
    "            else:\n",
    "                breakstart_nopunc = True\n",
    "            startpos = len(maintext[:startpos].split())-1\n",
    "        else:\n",
    "            startpos = len(maintext[:startpos].split())\n",
    "    if (endpos!=len(maintext)) and (maintext[endpos]!=\" \"):\n",
    "        if maintext[endpos-1] in puncs:\n",
    "            breakend_inclpunc = True\n",
    "        else:\n",
    "            breakend_nopunc = True\n",
    "    endpos  = len(maintext[:endpos].split())\n",
    "    returntext = pretext.split()[startpos:endpos]\n",
    "    if breakstart_inclpunc:\n",
    "        for i in range(len(returntext[0])):\n",
    "            if returntext[0][i] in puncs:\n",
    "                returntext[0] = returntext[0][i:]\n",
    "                break\n",
    "    elif breakstart_nopunc:\n",
    "        sawpunc = False\n",
    "        for i in range(len(returntext[0])):\n",
    "            if returntext[0][i] in puncs:\n",
    "                sawpunc = True\n",
    "            if sawpunc and (returntext[0][i] not in puncs):\n",
    "                returntext[0] = returntext[0][i:]\n",
    "                break\n",
    "    if breakend_inclpunc:\n",
    "        sawpunc = False\n",
    "        for i in range(len(returntext[-1])):\n",
    "            if returntext[-1][i] in puncs:\n",
    "                sawpunc = True\n",
    "            if sawpunc and (returntext[-1][i] not in puncs):\n",
    "                returntext[-1] = returntext[-1][:i]\n",
    "                break\n",
    "    elif breakend_nopunc:\n",
    "        for i in range(len(returntext[-1])):\n",
    "            if returntext[-1][i] in puncs:\n",
    "                returntext[-1] = returntext[-1][:i]\n",
    "                break\n",
    "    return \" \".join(returntext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidation\n",
    "postestdata['selected_text']=postestdata.apply(lambda s: endcorrect(s.text, startcorrect(\n",
    "    s.text, s.selected_text)), axis=1)\n",
    "negtestdata['selected_text']=negtestdata.apply(lambda s: endcorrect(s.text, startcorrect(\n",
    "    s.text, s.selected_text)), axis=1)\n",
    "postestdata['selected_text']=postestdata.apply(lambda s: reverseprocessing(s.text, s.original_text, \n",
    "                                                                           s.selected_text), axis=1)\n",
    "negtestdata['selected_text']=negtestdata.apply(lambda s: reverseprocessing(s.text, s.original_text, \n",
    "                                                                           s.selected_text), axis=1)\n",
    "testdata = testdata.merge(postestdata[['textID','selected_text']], on=\"textID\", how=\"left\").merge(\n",
    "    negtestdata[['textID', 'selected_text']], on=\"textID\", how=\"left\")\n",
    "testdata['selected_text'] = testdata.apply(lambda s: pd.Series([s.selected_text_x, s.selected_text_y, \n",
    "                                                                s.selected_text]).dropna().values[0], axis=1)\n",
    "testdata.drop(columns=[\"selected_text_x\", \"selected_text_y\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata[[\"textID\",\"selected_text\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
