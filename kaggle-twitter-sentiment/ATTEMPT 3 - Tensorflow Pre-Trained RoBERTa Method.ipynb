{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook attempts Facebook AI's pre-trained TFRoBERTa embeddings with token identification, question-answer, classification and regression heads on the base model. All modelling was run on a Kaggle GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import re\n",
    "import pickle\n",
    "import gc\n",
    "import nltk\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 72\n",
    "data = pd.read_csv(\"./data/train.csv\")\n",
    "testdata = pd.read_csv(\"./data/test.csv\")\n",
    "data.dropna(how=\"any\", inplace=True)\n",
    "#Attempt reproducibility\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_cleaner(word):\n",
    "    regex = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    replace = r'\\1\\2\\3'\n",
    "    if word==\"\": return word\n",
    "    if word==\"<3\": return \"LOVE\" #Independent emoticon disambiguation\n",
    "    for i in range(len(word)):\n",
    "        if word[i] in [\"!\",\".\",\"?\",\":\",\",\",\"`\",\"-\",\"=\"]: \n",
    "            return slang_cleaner(word[:i])+word[i]+slang_cleaner(word[i+1:])\n",
    "    if nltk.corpus.wordnet.synsets(word): return word\n",
    "    clean = regex.sub(replace, word)\n",
    "    if (word==clean): return word\n",
    "    else: return slang_cleaner(clean)\n",
    "def cleaner(sent):\n",
    "    #Two tokens (WEBSITE, VULGAR) are created and punctuation is spaced out\n",
    "    sent = \" \".join(np.vectorize(lambda s:\"WEBSITE\" if \"http\" in s or (\"www\" in s and \"com\" in s) else s)\n",
    "                    (np.array(sent.split())))\n",
    "    for punc in [\"\\!\",\"\\.\",\"\\?\",\"\\:\",\"\\;\",\"\\,\"]:\n",
    "        sent = re.sub(re.compile('(?:'+punc+'){2,}'),punc[1],sent)\n",
    "    sent = re.sub(\"[`]\",\"\\'\",sent)\n",
    "    sent = re.sub(re.compile('(?:\\*){2,}'),\"VULGAR\",sent)\n",
    "    return (\" \".join(np.vectorize(slang_cleaner)(np.array(sent.split())))).lower()\n",
    "for col in ['text','selected_text']:\n",
    "    data[col] = data[col].apply(lambda s: cleaner(s))\n",
    "data = data.loc[data.text.apply(lambda s: len(s))!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validitycheck(s):\n",
    "    target = s.selected_text.split()\n",
    "    source = s.text.split()\n",
    "    for i in range(len(source)):\n",
    "        if (source[i]==target[0]) and (source[i:i+len(target)]==target): return True\n",
    "    return False\n",
    "data = data.loc[data.apply(validitycheck, axis=1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> N-Gram Level Subtext Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the observations established in ATTEMPT 2 regarding the similarity between \"text\" and \"selected_text\" for neutral sentiments which makes it favorable to merely predict the entire text (at 0.92 accuracy). For positive and negative labels, we attempt classifier and regressor designs to predict subtexts, or predict Jaccardian similarity with subtexts. While these methods yield above-baseline results, superior performance was achieved with QA Approach below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 1: Classifier </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X(text):\n",
    "    ngrams, text = [], text.split()\n",
    "    for i in range(len(text)):\n",
    "        for j in range(i, len(text)):\n",
    "            ngrams.append(\" \".join(text[i:j+1]))\n",
    "    return np.array(ngrams)\n",
    "def Y(ngrams, selected_text):\n",
    "    return np.vectorize(lambda s: int(s==selected_text))(np.array(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = data.loc[data.sentiment==\"positive\", \"text\"].apply(lambda s: np.sum(np.arange(len(s.split())+1)))\n",
    "trainX = np.zeros(length.sum(), dtype=\"<U150\")\n",
    "trainY = np.zeros(length.sum(), dtype=np.float64)\n",
    "i=0\n",
    "for index, content in data.loc[data.sentiment==\"positive\"].iterrows():\n",
    "    ngrams = X(content.text)\n",
    "    trainX[i:i+len(ngrams)] = ngrams\n",
    "    trainY[i:i+len(ngrams)] =  Y(ngrams, content.selected_text)\n",
    "    i+=len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Undersampling - Create a 5% Sampling Ratio\n",
    "majority, minority = np.where(trainY==0)[0], np.where(trainY==1)[0]\n",
    "kp = np.random.choice(majority, int(0.05*len(majority)), replace=False)\n",
    "trainX = trainX[np.sort(np.append(minority, kp))]\n",
    "trainY = trainY[np.sort(np.append(minority, kp))]\n",
    "pd.Series(trainY).plot(kind=\"hist\", bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Labels\n",
    "MAX_LEN = 100\n",
    "input_ids = np.ones((len(trainX),MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((len(trainX),MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((len(trainX),MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(len(trainX)):\n",
    "    text1 = \" \"+\" \".join(trainX[k].split())\n",
    "    enc = tokenizer.encode(text1)\n",
    "    s_tok = sentiment_id[\"positive\"]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word-Level BERT LSTM ANN\n",
    "def dense_mdl(nunits=100, recdrp=0.05, l2reg=0.05, drp=0.05):\n",
    "    input_ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    x1 = tf.keras.layers.LSTM(nunits, recdrp=recdrp, kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n",
    "                              bias_regularizer=tf.keras.regularizers.l2(l2reg), return_sequences=False)(x[0])\n",
    "    x1 = tf.keras.layers.Dropout(drp)(x1)\n",
    "    final = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x1)\n",
    "    model = tf.keras.models.Model([input_ids, attention_mask, token_type_ids], final)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=\"binary_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = dense_mdl()\n",
    "mdl.fit([input_ids, attention_mask, token_type_ids], trainY, epochs=10, batch_size=50, verbose=True,\n",
    "       class_weight={0:1, 1:pd.Series(trainY).value_counts()[0]/pd.Series(trainY).value_counts()[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>STEP 2: Regressor </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Y(ngrams, subset):\n",
    "    def jaccard(str1, str2):\n",
    "        a, b = set(str1.lower().split()), set(str2.lower().split())\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    return np.vectorize(lambda s: jaccard(s, subset))(np.array(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence-Level BERT Dense ANN with Dual Input\n",
    "def dense_mdl(nunits=100, activation=\"relu\", l2reg=0.05, drp=0.05):\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    #Phrase analysis\n",
    "    input_ids1 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    attention_mask1 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    token_type_ids1 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    bert_model1 = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x1 = bert_model(input_ids1, attention_mask=attention_mask1, token_type_ids=token_type_ids1)\n",
    "    #Sentence analysis\n",
    "    input_ids2 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    attention_mask2 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    token_type_ids2 = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    bert_model2 = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x2 = bert_model2(input_ids2, attention_mask=attention_mask2, token_type_ids=token_type_ids2)\n",
    "    #Combined analysis\n",
    "    x = tf.concat([x1[1],x2[1]], axis=-1)\n",
    "    x = tf.keras.layers.Dense(nunits, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n",
    "                               bias_regularizer=tf.keras.regularizers.l2(l2reg))(x)\n",
    "    x = tf.keras.layers.Dropout(drp)(x)\n",
    "    final = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.models.Model([input_ids1, attention_mask1, token_type_ids1, input_ids2, \n",
    "                                   attention_mask2,token_type_ids2], final)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), loss=\"mean_squared_error\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HuggingFace Models </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is inspired by Prof. Chris Deotte's question-answer head and labelling approach with HuggingFace. More details on the Kaggle discussion forum. The code below achieved a 0.697 Jaccard on the public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 1: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labelling(info, training=True):\n",
    "    input_ids = np.ones((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    attention_mask = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    token_type_ids = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    start_tokens = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    end_tokens = np.zeros((info.shape[0],MAX_LEN),dtype='int32')\n",
    "    for k in range(info.shape[0]):\n",
    "        text1 = \" \"+\" \".join(info.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1) \n",
    "        offsets = []; idx=0\n",
    "        for t in enc.ids:\n",
    "            w = tokenizer.decode([t])\n",
    "            offsets.append((idx,idx+len(w)))\n",
    "            idx += len(w)\n",
    "        s_tok = sentiment_id[info.loc[k,'sentiment']]\n",
    "        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "        attention_mask[k,:len(enc.ids)+5] = 1\n",
    "        if training:\n",
    "            text2 = \" \".join(info.loc[k,'selected_text'].split())\n",
    "            idx = text1.find(text2)\n",
    "            chars = np.zeros((len(text1)))\n",
    "            chars[idx:idx+len(text2)]=1\n",
    "            if text1[idx-1]==' ': chars[idx-1] = 1\n",
    "            toks = []\n",
    "            for i,(a,b) in enumerate(offsets):\n",
    "                sm = np.sum(chars[a:b])\n",
    "                if sm>0: toks.append(i)\n",
    "            if len(toks)>0:\n",
    "                start_tokens[k,toks[0]+1] = 1\n",
    "                end_tokens[k,toks[-1]+1] = 1\n",
    "    return input_ids, attention_mask, token_type_ids, start_tokens, end_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Loss Functions and Model Heads Attempted\n",
    "def distance_weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    crossentropy = tf.reduce_sum(tf.math.multiply(tf.math.log(y_pred), tf.cast(y_true, dtype=tf.float32)), axis=-1)\n",
    "    distance = tf.abs(tf.math.argmax(y_true, axis=-1)-tf.argmax(y_pred, axis=-1))+1\n",
    "    return -tf.reduce_sum(tf.math.multiply(tf.math.sqrt(tf.cast(distance, dtype=tf.float32)), crossentropy))\n",
    "\n",
    "#Small Addendum to leverage Token Classification for Individual Sentences\n",
    "#bert_model = TFRobertaForTokenClassification.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "#x = bert_model(ids,attention_mask=att,token_type_ids=tok, training=training)\n",
    "#x1 = tf.keras.layers.Flatten()(x[0][:,:,-1])\n",
    "#x1 = tf.keras.layers.Activation('softmax')(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start-Index and End-Index Dual TFRoberta Model\n",
    "def build_model(drp=0.1, l2reg=0.00, activation=None, kinit=\"glorot_uniform\"):\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    config = RobertaConfig.from_pretrained(PATH+\"config-roberta-base.json\")\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    x1 = tf.keras.layers.Dropout(drp)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(1,1, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n",
    "                               bias_regularizer=tf.keras.regularizers.l2(l2reg), kernel_initializer=kinit)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    x2 = tf.keras.layers.Dropout(drp)(x[0])\n",
    "    x2 = tf.keras.layers.Conv1D(1,1, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n",
    "                               bias_regularizer=tf.keras.regularizers.l2(l2reg), kernel_initializer=kinit)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Label Training\n",
    "train = data.loc[data.sentiment==\"positive\"].reset_index(drop=True)\n",
    "input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = generate_labelling(train, True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model()\n",
    "model.fit([input_ids, attention_mask, token_type_ids], [start_tokens, end_tokens], \n",
    "        epochs=3, batch_size=64, verbose=True)\n",
    "model.save_weights(\"posroberta.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Label Training\n",
    "train = data.loc[data.sentiment==\"negative\"].reset_index(drop=True)\n",
    "input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = generate_labelling(train, True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model()\n",
    "model.fit([input_ids, attention_mask, token_type_ids], [start_tokens, end_tokens], \n",
    "        epochs=3, batch_size=64, verbose=True)\n",
    "model.save_weights(\"negroberta.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 2: Model Predictions </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neutral Labels\n",
    "testdata = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
    "testdata['original_text'] = testdata['text'].copy()\n",
    "testdata['text'] = testdata['text'].apply(lambda s: cleaner(s))\n",
    "testdata['selected_text'] = np.nan\n",
    "testdata.loc[testdata.sentiment==\"neutral\", \"selected_text\"] = testdata.loc[\n",
    "    testdata.sentiment==\"neutral\", \"original_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SubText Generation Function\n",
    "def subtext(info, input_ids, attention_mask, token_type_ids, model):\n",
    "    begin, end = model.predict([input_ids, attention_mask, token_type_ids])\n",
    "    for k in range(len(info)):\n",
    "        text1 = \" \"+\" \".join(info.loc[info.index[k],'text'].split())\n",
    "        enc, st = tokenizer.encode(text1), None\n",
    "        a = np.argmax(begin[k,])\n",
    "        b = np.argmax(end[k,])\n",
    "        if a>b:\n",
    "            if np.max(end[k,])>np.max(begin[k,]):\n",
    "                st = tokenizer.decode(enc.ids[:b])\n",
    "            else:\n",
    "                st = tokenizer.decode(enc.ids[a-1:])\n",
    "        else:\n",
    "            st = tokenizer.decode(enc.ids[a-1:b]) \n",
    "        info.loc[info.index[k], \"selected_text\"] = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Labels\n",
    "postestdata = testdata.loc[testdata.sentiment==\"positive\"].reset_index(drop=True)\n",
    "input_ids, attention_mask, token_type_ids, _, _ = generate_labelling(postestdata, False)\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model()\n",
    "model.load_weights(\"posroberta.h5\")\n",
    "subtext(postestdata, input_ids, attention_mask, token_type_ids, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Labels\n",
    "negtestdata = testdata.loc[testdata.sentiment==\"negative\"].reset_index(drop=True)\n",
    "input_ids, attention_mask, token_type_ids, _, _ = generate_labelling(negtestdata, False)\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model()\n",
    "model.load_weights(\"negroberta.h5\")\n",
    "subtext(negtestdata, input_ids, attention_mask, token_type_ids, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coherence Corrections\n",
    "def startcorrect(fulltext, subtext):\n",
    "    subtext = \" \".join(subtext.split())\n",
    "    i = fulltext.find(subtext)\n",
    "    k = i\n",
    "    while (k!=0) and (fulltext[k]!=\" \"): k-=1\n",
    "    return fulltext[k:i]+subtext\n",
    "def endcorrect(fulltext, subtext):\n",
    "    subtext = \" \".join(subtext.split())\n",
    "    i = fulltext.find(subtext)+len(subtext)\n",
    "    k = i\n",
    "    while (k!=len(fulltext)) and (fulltext[k]!=\" \"): k+=1\n",
    "    return subtext+fulltext[i:k]\n",
    "def reverseprocessing(posttext, pretext, subtext):\n",
    "    posttext = posttext.split()\n",
    "    subtext = subtext.split()\n",
    "    for i in range(len(posttext)):\n",
    "        if (posttext[i:i+len(subtext)]==subtext):\n",
    "            return \" \".join(pretext.split()[i:i+len(subtext)])\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidation\n",
    "postestdata['selected_text']=postestdata.apply(\n",
    "    lambda s: endcorrect(s.text, startcorrect(s.text, s.selected_text)), axis=1)\n",
    "negtestdata['selected_text']=negtestdata.apply(\n",
    "    lambda s: endcorrect(s.text, startcorrect(s.text, s.selected_text)), axis=1)\n",
    "postestdata['selected_text']=postestdata.apply(\n",
    "    lambda s: reverseprocessing(s.text, s.original_text, s.selected_text), axis=1)\n",
    "negtestdata['selected_text']=negtestdata.apply(\n",
    "    lambda s: reverseprocessing(s.text, s.original_text, s.selected_text), axis=1)\n",
    "testdata = testdata.merge(postestdata[['textID','selected_text']], on=\"textID\", how=\"left\").merge(\n",
    "    negtestdata[['textID', 'selected_text']], on=\"textID\", how=\"left\")\n",
    "testdata['selected_text'] = testdata.apply(lambda s: pd.Series([s.selected_text_x, s.selected_text_y, \n",
    "                                                                s.selected_text]).dropna().values[0], axis=1)\n",
    "testdata.drop(columns=[\"selected_text_x\", \"selected_text_y\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata[[\"textID\",\"selected_text\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
